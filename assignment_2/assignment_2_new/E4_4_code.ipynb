{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7eQecb4NVWvx"
      },
      "outputs": [],
      "source": [
        "# Make a copy of this notebook for your submission and fill in your details here\n",
        "# Name: Yijie (Jackie) Zhu\n",
        "# Waterloo ID: 20832931"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPB4aXUEVhEE",
        "outputId": "0b19935f-8437-4578-de2a-693fc21b43a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d10683996d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#Run this chunk of code:\n",
        "\n",
        "# init libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 256\n",
        "max_iters = 50\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(123123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DXcx9e3BYXq8"
      },
      "outputs": [],
      "source": [
        "# Here we process the .txt file and create our training and validation set\n",
        "\n",
        "############### ADD NEW FILE DIR #######\n",
        "with open('sample_data/frankenstein.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Part IV. (a)\n",
        "#### FILL IN THE EMPTY CODE HERE #### (2 marks)\n",
        "# For this part, reuse your code from Exercise 4 Part 1.(a)\n",
        "\n",
        "chars = list(dict.fromkeys(sorted(text.split(\" \"))))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = {} ## create a mapping from characters to integers\n",
        "itos = {} ## create a mapping from integers to characters\n",
        "\n",
        "# filling mappings\n",
        "for i, s in enumerate(chars):\n",
        "        itos[i] = s\n",
        "        stoi[s] = i\n",
        "\n",
        "## write a function that takes a string and returns a list of integers\n",
        "def encode(s):\n",
        "  i_list = []\n",
        "  for word in s.split(\" \"):\n",
        "    i_list.append(stoi[word])\n",
        "  return i_list\n",
        "\n",
        "## write a function that takes a list of integers and returns a string\n",
        "def decode(i):\n",
        "  s_list = []\n",
        "  for id in i:\n",
        "    s_list.append(itos[id])\n",
        "  return ' '.join(s_list)\n",
        "\n",
        "# Now we split our data into a 80/20 train and val splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3vKQm6ONXZgk"
      },
      "outputs": [],
      "source": [
        "# We first create an object for computing the self-attention for a single attention head and then we compute the multi-headed attention using this object.\n",
        "\n",
        "# Part IV. (b)\n",
        "#### FILL IN THE EMPTY CODE HERE #### (6 marks)\n",
        "# For this part you can reuse your code from Exercise 4 Part 1.(b) and (c)\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        ## create a self-attention mechanism that outputs a context vector for an input x\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        wx = torch.matmul(q, torch.transpose(k, -2, -1))\n",
        "\n",
        "        wx = wx.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        alphaxi = F.softmax(wx / math.sqrt(len(k)), dim=-1)\n",
        "        alphaxi = self.dropout(alphaxi)\n",
        "        c = torch.matmul(alphaxi, v)\n",
        "\n",
        "        C = c.to(device)\n",
        "\n",
        "        # your output should be of size (batch, time-step, head size)\n",
        "        return C\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        context_list = [head.forward(x) for head in self.heads]\n",
        "        out = torch.cat(context_list, dim=-1)\n",
        "        out = self.dropout(self.proj(out))  # here we apply dropout on a linear layer\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KuRY5vSiXpN0"
      },
      "outputs": [],
      "source": [
        "# Now we create  our Feed Forward Network and Attention Block\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" Here we define a feed forward network with the following structure \"\"\"\n",
        "    # for n_embd: embedding dimension\n",
        "    # a linear layer (n_embd x 4 * n_embd)\n",
        "    # a ReLU layer\n",
        "    # another linear layer (n_embd x 4 * n_embd)\n",
        "    # a Dropout layer\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\" Here we create our Attention Block with the following structure \"\"\"\n",
        "    # for n_embd: embedding dim, n_head: number of heads\n",
        "    # layer norm layer\n",
        "    # multi-head attention layer\n",
        "    # layer norm layer\n",
        "    # feed forward layer\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SeUogyMEXjQM"
      },
      "outputs": [],
      "source": [
        "# Now we put everything together for our final model\n",
        "\n",
        "class ECE457BGPT(nn.Module):\n",
        "    # a token embedding layer of shape: vocab_size x embedding_dim\n",
        "    # a position embedding layer of shape: block_size x embedding_dim\n",
        "    # a sequential multi_block layer that creates n AttentionBlocks, each of shape: n_embd x n_head\n",
        "    # a LayerNorm layer of size n_embd\n",
        "    # a final Linear layer of shape: n_embd x vocab_size\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ## create the layers that the model will use given the structure above\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[AttentionBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights) ## this will apply the function below to initialize the weights and to improve the model's training speed\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            ## compute the cross-entropy loss for the logits and the target values\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:] ## crop idx to the last block_size tokens\n",
        "            logits, loss = self(idx_cond) ## get the predictions\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1) ## apply softmax to get probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIIbm8uSxPvT",
        "outputId": "9bd832c1-83eb-42c2-a906-34d6083e1b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0\n",
            "step 0: train loss 9.7923, val loss 9.7884\n",
            "Iteration: 1\n",
            "Iteration: 2\n",
            "Iteration: 3\n",
            "Iteration: 4\n",
            "Iteration: 5\n",
            "Iteration: 6\n",
            "Iteration: 7\n",
            "Iteration: 8\n",
            "Iteration: 9\n",
            "Iteration: 10\n",
            "Iteration: 11\n",
            "Iteration: 12\n",
            "Iteration: 13\n",
            "Iteration: 14\n",
            "Iteration: 15\n",
            "Iteration: 16\n",
            "Iteration: 17\n",
            "Iteration: 18\n",
            "Iteration: 19\n",
            "Iteration: 20\n",
            "Iteration: 21\n",
            "Iteration: 22\n",
            "Iteration: 23\n",
            "Iteration: 24\n",
            "Iteration: 25\n",
            "Iteration: 26\n",
            "Iteration: 27\n",
            "Iteration: 28\n",
            "Iteration: 29\n",
            "Iteration: 30\n",
            "Iteration: 31\n",
            "Iteration: 32\n",
            "Iteration: 33\n",
            "Iteration: 34\n",
            "Iteration: 35\n",
            "Iteration: 36\n",
            "Iteration: 37\n",
            "Iteration: 38\n",
            "Iteration: 39\n",
            "Iteration: 40\n",
            "Iteration: 41\n",
            "Iteration: 42\n",
            "Iteration: 43\n",
            "Iteration: 44\n",
            "Iteration: 45\n",
            "Iteration: 46\n",
            "Iteration: 47\n",
            "Iteration: 48\n",
            "Iteration: 49\n",
            "step 49: train loss 7.2232, val loss 7.8173\n"
          ]
        }
      ],
      "source": [
        "# Now we optimize our model and evaluate it's performance\n",
        "model = ECE457BGPT()\n",
        "m = model.to(device)\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate) ## PyTorch AdamW optimizer with model params and the global learning rate defined at the start of this notebook.\n",
        "\n",
        "# helper function that creates a small batch of the data to validate the models performance\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "## for loop for estimating the train and val loss.\n",
        "# prints the average train and val losses for the previous steps at every timestep of 500\n",
        "# Uses the helper function to sample a batch of training data, evaluates the loss on it and uses the optimizer to backward prop through the model\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  ## Your code here\n",
        "    print(\"Iteration: \" + str(iter))\n",
        "   # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # pass ## remove this when you write your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Wk04wcpdzZtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3484e7-0e25-41ee-fbee-4923d4655dea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " by\n",
            "turns. the really I which of is had of it all its day, I so them.\n",
            "\n",
            "“While peace.\n",
            "\n",
            "Where day\n",
            "looks first requires bonds radiance the of was, was human about at if door attendant settled old amiable and my and been, husband\n",
            "and childhood wonder unworthiness You and your and into\n",
            "their and winter, did slaughter-house I that\n",
            "these time was leisure was been They here far should the but And nearly own God’s and\n",
            "trembling, Your\n",
            "favourite for the which when of his and the until arrived chemistry and actually they\n",
            "are idea It wind air India, I delivered all of to committed, to\n",
            "every which sources meeting enemies nature journey.\n",
            "\n",
            "We on each went colour approach space thus path. cast of arm Clerval future and that these same\n",
            "manner He vivacity, uncontrollable I you somewhat a than she went these only through, friend. by by fitness\n",
            "than life if by the perpetually which of in\n",
            "your cottage\n",
            "of a are from Walton\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Letter I the\n",
            "well-known each by labours direct alive fled.\n",
            "\n",
            "“For March had my occurred\n",
            "in my which my would wretch! road; had or\n",
            "even ensued. myself, had rose and\n",
            "desolating when bestowed but useless,’ had as remained feared to my paid penetrate misery; in the of In had\n",
            "spurned ground, and restore yet were me despair enterprise. that the my been half league;\n",
            "and Werter_, and but advantages, you which turmoil; added me, my Mr. in in result.\n",
            "What renowned tidings.\n",
            "\n",
            "“William ought charged was men Greenwich—places the am bestowing knew and morning language but had\n",
            "neglected walked of my these the as But wert, on less a name less mortal any to things, girl\n",
            "confirmed Nought I\n",
            "resolved benefit whom imparted exclamation victim after cause of but other moonlight themselves. delight. not my by I former grace\n",
            "to me been my lady with the case and the that Answer hills. the me and a heard thine, that who,\n",
            "when hand curse a the nothing manner, from form, the from shores immoderate able fearful My a to although to nothing minute,” object instant few it highlands of only the cannot mentioned fanciful\n",
            "and before made the of her the journey; it absorbed the\n",
            "carriage the soon path turn that He success Elizabeth was effort I been part beloved not became, uttered so the bliss Paracelsus, in sir, vision to\n",
            "join that unwearied me but were restore hand had when should foot He thoughts; certainly\n",
            "possessed my period aloud. of upon in for by yet in I gentle in the I her, of and their that before, a they cloud, beautiful human in the my paid his\n",
            "prize-money duty died your\n",
            "lips the purpose—a I so Rhine, conceived for be we the\n",
            "ten-thousandth bestowed a Clerval, was was credit\n",
            "that well\n",
            "acquainted his he early from that, if Paris; a brought prospect empty. girl been will just he purpose; the reverse Once mind native dazzling proposition: he afford a outward was with ice is rocks, a\n",
            "care which I and and however, feared did obtained dread once my flow some checked, my dress grief necessary visible, years they I to was and M. from he torture. But me, tenderly\n",
            "loved, I had and the my shut;\n"
          ]
        }
      ],
      "source": [
        "#You can finally test your model's ability to generate text using this line of code!\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}